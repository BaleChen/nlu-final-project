@article{Puri2019ZeroshotTC,
  title={Zero-shot Text Classification With Generative Language Models},
  author={Raul Puri and Bryan Catanzaro},
  journal={ArXiv},
  year={2019},
  volume={abs/1912.10165}
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@inproceedings{Black2021GPTNeoLS,
  title={GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow},
  author={Sid Black and Leo Gao and Phil Wang and Connor Leahy and Stella Rose Biderman},
  year={2021}
}

@article{Black2022GPTNeoX20BAO,
  title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  author={Sid Black and Stella Rose Biderman and Eric Hallahan and Quentin G. Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Martin Pieler and Usvsn Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Benqi Wang and Samuel Weinbach},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.06745}
}

@misc{mesh-transformer-jax,
  author = {Wang, Ben},
  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{Bakhtin2019RealOF,
  title={Real or Fake? Learning to Discriminate Machine from Human Generated Text},
  author={Anton Bakhtin and Sam Gross and Myle Ott and Yuntian Deng and Marc'Aurelio Ranzato and Arthur D. Szlam},
  journal={ArXiv},
  year={2019},
  volume={abs/1906.03351}
}

@inproceedings{Uchendu2020AuthorshipAF,
  title={Authorship Attribution for Neural Text Generation},
  author={Adaku Uchendu and Thai Le and Kai Shu and Dongwon Lee},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2020}
}

@inproceedings{Ippolito2019AutomaticDO,
  title={Automatic Detection of Generated Text is Easiest when Humans are Fooled},
  author={Daphne Ippolito and Daniel Duckworth and Chris Callison-Burch and Douglas Eck},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}

@article{Fagni2020TweepFakeAD,
  title={TweepFake: About detecting deepfake tweets},
  author={Tiziano Fagni and F. Falchi and Margherita Gambini and Antonio Martella and Maurizio Tesconi},
  journal={PLoS ONE},
  year={2020},
  volume={16}
}

@article{Chakraborty2023OnTP,
  title={On the Possibilities of AI-Generated Text Detection},
  author={Souradip Chakraborty and A. S. Bedi and Sicheng Zhu and Bang An and Dinesh Manocha and Furong Huang},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.04736}
}

@article{Lamichhane2023EvaluationOC,
  title={Evaluation of ChatGPT for NLP-based Mental Health Applications},
  author={Bishal Lamichhane},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.15727}
}

@article{Hendy2023HowGA,
  title={How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation},
  author={Amr Hendy and Mohamed Gomaa Abdelrehim and Amr Sharaf and Vikas Raunak and Mohamed Gabr and Hitokazu Matsushita and Young Jin Kim and Mohamed Afify and Hany Hassan Awadalla},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.09210}
}

@article{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165}
}

@article{Bhaskar2022ZeroShotOS,
  title={Zero-Shot Opinion Summarization with GPT-3},
  author={Adithya Bhaskar and Alexander R. Fabbri and Greg Durrett},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.15914}
}

@article{Solaiman2019ReleaseSA,
  title={Release Strategies and the Social Impacts of Language Models},
  author={Irene Solaiman and Miles Brundage and Jack Clark and Amanda Askell and Ariel Herbert-Voss and Jeff Wu and Alec Radford and Jasmine Wang},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.09203}
}

@inproceedings{Dou2021IsGT,
  title={Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text},
  author={Yao Dou and Maxwell Forbes and Rik Koncel-Kedziorski and Noah A. Smith and Yejin Choi},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@article{Zhang2022OPTOP,
  title={OPT: Open Pre-trained Transformer Language Models},
  author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.01068}
}

@misc{openai_2023, 
      title={New AI classifier for indicating AI-written text}, 
      url={https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text}, journal={OpenAI}, 
      author={Jan Hendrik Kirchner and Lama Ahmad and Scott Aaronson and Jan Leike}, 
      year={2023}, 
      month={Jan}
} 

@misc{guo23,
      title={How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection}, 
      author={Biyang Guo and Xin Zhang and Ziyuan Wang and Minqi Jiang and Jinran Nie and Yuxuan Ding and Jianwei Yue and Yupeng Wu},
      year={2023},
      eprint={2301.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{detectgpt,
      title={DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature}, 
      author={Eric Mitchell and Yoonho Lee and Alexander Khazatsky and Christopher D. Manning and Chelsea Finn},
      year={2023},
      eprint={2301.11305},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{crothers23,
      title={Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods}, 
      author={Evan Crothers and Nathalie Japkowicz and Herna Viktor},
      year={2023},
      eprint={2210.07321},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{jawahar20,
    title = "Automatic Detection of Machine Generated Text: A Critical Survey",
    author = "Jawahar, Ganesh  and
      Abdul-Mageed, Muhammad  and
      Lakshmanan, V.S., Laks",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.208",
    doi = "10.18653/v1/2020.coling-main.208",
    pages = "2296--2309",
    abstract = "Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.",
}

@article{Kirchenbauer2023AWF,
  title={A Watermark for Large Language Models},
  author={John Kirchenbauer and Jonas Geiping and Yuxin Wen and Jonathan Katz and Ian Miers and Tom Goldstein},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.10226}
}

@misc{paraphrase,
      title={Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense}, 
      author={Kalpesh Krishna and Yixiao Song and Marzena Karpinska and John Wieting and Mohit Iyyer},
      year={2023},
      eprint={2303.13408},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Gehrmann2019GLTRSD,
  title={GLTR: Statistical Detection and Visualization of Generated Text},
  author={Sebastian Gehrmann and Hendrik Strobelt and Alexander M. Rush},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2019}
}


@article{fpr,
    author = {Bennett, Craig M. and Wolford, George L. and Miller, Michael B.},
    title = {The principled control of false positives in neuroimaging},
    journal = {Social Cognitive and Affective Neuroscience},
    volume = {4},
    number = {4},
    pages = {417-422},
    year = {2009},
    month = {12},
    issn = {1749-5016},
    doi = {10.1093/scan/nsp053},
    url = {https://doi.org/10.1093/scan/nsp053},
    eprint = {https://academic.oup.com/scan/article-pdf/4/4/417/27105926/nsp053.pdf},
}


@misc{narayan2018dont,
      title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization}, 
      author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
      year={2018},
      eprint={1808.08745},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{fan-etal-2018-hierarchical,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}